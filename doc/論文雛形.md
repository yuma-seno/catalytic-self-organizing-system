# 触媒的自己組織化システム (CSOS) による省メモリ指向エッジAIアーキテクチャ

**Memory-Oriented Edge AI Architecture based on a Catalytic Self-Organizing System (CSOS): Buffer-less Inference and Active Suppression**

**著者 (Author):**
* **妹尾 悠真 (Yuma Senoo)** - 個人研究者 (Independent Researcher)

---

## Abstract (Summary)
Deploying deep learning models on edge devices with severely limited memory resources (TinyML) remains a significant challenge. While conventional models like CNNs and Transformers require input buffers (sliding windows) that consume valuable on-chip SRAM linearly with input length $O(T)$, biological systems process information via fixed-size concentration dynamics without explicit buffers. Inspired by this, we present the "Catalytic Self-Organizing System (CSOS)," a bio-inspired architecture that encodes temporal context into a compact, fixed-size internal state vector.

We empirically evaluated a dense instantiation of CSOS on image classification (MNIST), control (CartPole), and wake-word detection. On MNIST, a CSOS model with 32 internal dimensions achieved **95.02% accuracy using only ~30% of the parameters** required by a comparable MLP baseline (which achieved 97.95%). This suggests that CSOS can learn highly compressed representations. In a wake-word task, CSOS exhibited a characteristic **"active suppression"** behavior, where internal reaction energy spontaneously increases to reject non-target patterns, a dynamic feature distinct from standard RNNs. Although the current $O(N^3)$ computational cost of dense interactions remains a bottleneck, we discuss how this offers a trade-off: minimizing runtime SRAM usage ($O(1)$ scaling) at the cost of computation, which is suitable for specific memory-constrained hardware.

### 要約（日本語）
深層学習モデルを極小メモリ環境（TinyML）へ展開する際、時系列データを扱うための入力バッファ（スライディングウィンドウ）がオンチップSRAMを圧迫するという課題がある。本研究では、生物の細胞内シグナル伝達系に着想を得た「触媒的自己組織化システム（CSOS）」を提案する。CSOSは、入力履歴を固定長の濃度ベクトル上の非線形反応ダイナミクスとして符号化することで、入力長 $T$ に依存しない $O(1)$ のSRAM消費量で動作する**バッファレス推論**を実現する。

実験において、MNIST分類では32次元のCSOSが、比較対象のMLPに対し**約30%のパラメータ数で95%超の正答率**を達成し、高いパラメータ効率と表現圧縮能力を示した。また、音声コマンド検出（Wake Word）においては、ターゲット以外の入力に対して内部反応エネルギーが増大し、出力を自律的に抑制する**「能動的抑制（Active Suppression）」**現象が観測された。現行の密結合モデルは $O(N^3)$ の演算コストを要するが、本研究は「SRAM消費の最小化」を最優先するアーキテクチャとしての可能性を示し、将来的な疎行列化による計算量削減の指針とともに報告する。

**Keywords:**
TinyML, memory efficiency, artificial chemistry, dynamical systems, active suppression, buffer-less architecture

---

## 1. まえがき
IoTデバイスやマイクロコントローラ（MCU）上のAI推論（TinyML）において、最も逼迫する資源は「実行時の一時メモリ（SRAM）」である [Banbury 21]．Flashメモリ（ストレージ）は数MB確保できる場合でも、SRAMは数百KB程度に限られることが多い．しかし、CNNやTransformer [Vaswani 17] で時系列データを処理する場合、過去の入力フレームを保持するスライディングウィンドウ型バッファや、巨大な中間特徴マップが必要となり、これらがSRAMを消費する主要因となっている [Warden 19]．

これに対し、RNNやLSTM [Hochreiter 97] は固定長の隠れ状態を持つため、入力長に依存しない $O(1)$ のメモリスケーリングが可能である．しかし、これらは通常「ブラックボックス」的な行列演算として扱われ、その内部ダイナミクスを利用した誤検知の抑制や、生物的なエネルギー効率の議論は十分になされていない．

本研究では、生物の化学反応系を模倣したアーキテクチャ「CSOS (Catalytic Self-Organizing System)」を提案する [Suzuki 09]．CSOSは以下の2点において、既存のTinyML手法に対するオルタナティブを提供する．
1.  **徹底したSRAM節約:** 入力バッファを持たず、極めて低次元（例: 32〜64次元）の濃度ベクトルのみを更新し続けることで、SRAM使用量を最小化する．
2.  **動的なロバスト性:** 化学反応の「平衡」や「競合」のアナロジーにより、未知の入力パターンに対してシステム全体が反応しにくくなる（あるいは強く反応して棄却する）といった、力学系特有のロバスト性を活用する．

本稿では、画像分類、制御、音声認識タスクを通じ、CSOSの基本性能と「パラメータ効率」、および「能動的抑制」メカニズムについて検証する．

また、本論文で行った実験に利用したコードは以下のGitHubリポジトリで公開している．興味のある読者は参照されたい．
* GitHub Repository:https://github.com/yuma-seno/catalytic-self-organizing-system/

---

## 2. 提案手法

### 2.1 触媒的自己組織化システム (CSOS)
CSOSは、非負の濃度ベクトル $\mathbf{x}_t \in \mathbb{R}_{\ge 0}^N$ を状態として持つ離散時間力学系である．

1.  **注入と混合:** 外部入力 $\mathbf{u}_t$ はエンコードされ、減衰した過去の状態と混合される．
    $$\tilde{\mathbf{x}} = (1 - \lambda)\mathbf{x}_{t-1} + W_{in}(\mathbf{u}_t)$$
2.  **触媒反応:** 3階の相互作用テンソル $\mathcal{W} \in \mathbb{R}^{N \times N \times N}$ を用いて反応項を計算する．
    $$r_k = \sum_{i,j} \mathcal{W}_{kij} \, \tilde{x}_i \, \tilde{x}_j$$
3.  **状態更新:** 反応生成物を加算し、活性化関数 $\phi$ を適用する．
    $$\mathbf{x}_t = \phi(\tilde{\mathbf{x}} + \alpha \cdot \mathbf{r})$$

ここで $\phi(\cdot)$ は、非負制約（ReLU）と、ベクトルの総和（またはノルム）を一定に保つ正規化処理を含む．これにより、状態 $\mathbf{x}_t$ は常に「閉じた系の中の濃度分布」として振る舞う．

**計算量に関する課題と展望:**
上記の反応項計算は、密結合テンソルの場合 $O(N^3)$ の積和演算を要する．$N=64$ の場合、1ステップあたり約26万回の演算となり、通常のRNN（$O(N^2)$）と比較して計算コストが高い．しかし、実際の生物学的反応ネットワークは極めて**スパース（疎）**である．本研究は概念実証のため密結合（Dense）実装を用いるが、実用時には $\mathcal{W}$ の大部分を0とする疎行列演算や、テンソル分解手法を適用することで、計算量を $O(N)$ 〜 $O(N^2)$ に削減可能である．今回は「SRAMメモリの極小化」に焦点を当て、計算量の最適化は今後の課題とする．

### 2.2 メモリ指向アーキテクチャとしての特性
CSOSは、モデルパラメータ（Flashメモリ）と実行時状態（SRAM）のトレードオフにおいて、**「SRAMの圧倒的節約」**に特化した特性を持つ．
* **SRAM (Runtime):** 入力長 $T$ に依存せず、常に $N \times 4$ バイト（float32）のみを保持すればよい．$N=32$ ならわずか 128 バイトである．
* **Flash (Storage):** $O(N^3)$ のパラメータを持つため、CNN等に比べて大きくなりやすいが、近年のMCUはSRAMに比べてFlash容量には余裕がある傾向（例: SRAM 256KB vs Flash 1MB）があるため、このトレードオフはTinyMLの文脈では合理的である．

---

## 3. 実験と結果

### 3.1 実験1：画像分類 (MNIST) - パラメータ効率の検証
**目的:** CSOSが高い情報圧縮能力を持ち、少ないパラメータ数で実用的な精度を出せるかを検証する．

**設定:**
* **CSOS:** 基底数 $N=32$．総パラメータ数 **約 8.0 万**．
* **Baseline (MLP):** 隠れ層 256-256．総パラメータ数 **約 26.7 万**．
* **学習:** 両者とも10エポックのみ学習．

**結果:**

**表1 MNISTにおけるパラメータ効率と精度の比較**

| モデル | パラメータ数 (Flash目安) | Test Accuracy | 備考 |
| :--- | :---: | :---: | :--- |
| **CSOS ($N=32$)** | **約 8.0 万 (Low)** | **95.02%** | **MLPの30%の容量で95%精度** |
| MLP (256-256) | 約 26.7 万 (High) | 97.95% | 精度は高いがサイズは大 |

CSOSは、MLPと比較して**約30%のパラメータ規模**であるにもかかわらず、95%という実用的な精度を維持した．これは、高次の相互作用（$\mathcal{W}$）が低次元空間内に豊富な表現力を生み出しており、極めてコンパクトなモデルでタスクを解ける可能性を示唆している．

### 3.2 実験2：音声コマンド検出 (Wake Word) - 能動的抑制の観測
**目的:** バッファレスな系列処理能力と、誤検知に対する挙動を検証する．

**[TODO: データセットの強化について]**
*本実験では予備検証として gTTS による合成音声を用いたが、より厳密な評価のためには Google Speech Commands Dataset 等の実音声データセットを用いた再検証が必要である．以下は合成データに基づく初期的な挙動解析の結果である．*

**設定:**
* ターゲット: "System Start"
* 非ターゲット: "System Stop", "Music Start", ノイズ等
* モデル: CSOS ($N=128$) vs MLP Baseline

**結果と「能動的抑制」:**
学習済みCSOSに対し、類似しているが異なるコマンド（例: "System Stop"）を入力した際、内部状態ベクトルの$L_2$ノルム（反応エネルギー）が、ターゲット入力時と比較して**一時的に増大し、その後急激に別のアトラクタへ収束する（出力確率を下げる）**挙動が観測された．

| 入力コマンド | 判定結果 | 反応ノルム挙動 (定性的) |
| :--- | :--- | :--- |
| **System Start** | **検知 (Target)** | 安定的に推移し、発火閾値を超える |
| System Stop | 棄却 (Reject) | **過渡的に増大し、抑制される (Active Suppression)** |

これは、RNNやGRUのようなゲート機構による制御とは異なり、非線形力学系が持つ「外れ値に対する反発」あるいは「エネルギー障壁」として機能していると解釈できる．この特性は、ノイズや未知語に対するロバストなWake Word検出器にとって有利な性質である．

---

## 4. 考察

### 4.1 なぜRNN/GRUではなくCSOSなのか？
RNNやGRUも $O(1)$ のSRAMスケーリングを持つが、CSOSは以下の点で異なる利点を持つ．
1.  **解釈可能性と能動的抑制:** 実験2で観測されたように、CSOSは内部エネルギー（ノルム）の変化として誤検知のプロセスを可視化できる．これにより、「なぜ誤検知したか」を力学的に解析可能である．
2.  **パラメータ効率:** 実験1で示されたように、高次のテンソル相互作用により、極めて少ないニューロン数（$N=32$）で複雑な境界を学習できる．これは、「ニューロン数＝SRAM使用量」に直結するため、メモリ制約下で有利に働く．

### 4.2 SRAM vs Flash のトレードオフ
CSOS ($N=64$) は、計算量が $O(N^3)$ と重く、Flashメモリ（パラメータ）も $N^3$ で増大する．しかし、TinyMLにおける最大のボトルネックは**「数KB〜数十KBしかないSRAM」**である場合が多い．
CNNが中間バッファとしてSRAMを大量消費するのに対し、CSOSは計算リソース（CPUサイクル）とFlash容量を「支払い」として、SRAMを極限まで節約するアーキテクチャと言える．したがって、クロック周波数には余裕があるがメモリが極端に少ないデバイスにおいて、CSOSは最適な選択肢となりうる．

---

## 5. 今後の課題 (Future Work)

本研究は概念実証（PoC）段階であり、実用化に向けて以下の課題に取り組む必要がある．

1.  **[TODO] 実音声データでのベンチマーク:** 合成音声ではなく、Speech Commands Dataset を用いて、GRU/LSTM との誤検知率（FAR）比較を行う．
2.  **[TODO] 疎行列化 (Sparsity) の実装:** 現在の $O(N^3)$ 密結合を、生物同様のスパース結合に置き換え、計算コストを $O(N)$ オーダまで削減する．
3.  **[TODO] 実機 (MCU) での消費電力測定:** Arduino Nano 33 BLE Sense 等に実装し、理論通りのSRAM使用量と、計算負荷による電力消費のバランスを実測する．

---

## 参考文献
[Vaswani 17] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł. and Polosukhin, I.: Attention is all you need, *Advances in Neural Information Processing Systems (NIPS)*, Vol. 30, pp. 5998–6008 (2017).

[Suzuki 09] Suzuki, H. and Dittrich, P.: Artificial Chemistry, *Artificial Life*, Vol. 15, No. 2, pp. 1-3 (2009).

[Hochreiter 97] Hochreiter, S. and Schmidhuber, J.: Long short-term memory, *Neural Computation*, Vol. 9, No. 8, pp. 1735-1780 (1997).

[Jaeger 04] Jaeger, H. and Haas, H.: Harnessing nonlinearity: Predicting chaotic systems and saving energy in wireless communication, *Science*, Vol. 304, No. 5667, pp. 78-80 (2004).

[Warden 19] Warden, P. and Situnayake, D.: *TinyML: Machine Learning with TensorFlow Lite on Arduino and Ultra-Low-Power Microcontrollers*, O'Reilly Media (2019).

[Lane 16] Lane, N. D., Bhattacharya, S., Georgiev, P., Forlivesi, C., Jiao, L., Qendro, L. and Kawsar, F.: DeepX: A resource-efficient deep learning framework for mobile deep vision, *Proceedings of the 14th ACM/IEEE International Conference on Information Processing in Sensor Networks (IPSN)*, pp. 353-366 (2016).

[Lukoševičius 12] Lukoševičius, M.: A practical guide to applying echo state networks, in *Neural Networks: Tricks of the Trade*, Springer, pp. 659-686 (2012).

[Banbury 21] Banbury, C. R., Reddi, V. J., Lam, M., Fu, W., Fazel, A., Holleman, J., Huang, X., Hurtado, R., Kanter, D., Lokhmotov, A., et al.: Benchmarking TinyML Systems: Challenges and Direction, *Proceedings of the TinyML Research Symposium (arXiv:2003.04821)* (2021).

---

## 著者紹介

**妹尾 悠真 (Yuma Senoo)**
生物模倣型人工知能（Bio-inspired AI）、特に人工化学（Artificial Chemistry）と非線形力学系を用いた新しい学習アーキテクチャの構築に関心を持つ個人研究者．既存の深層学習が抱える計算資源と解釈性の課題に対し、生物学的制約を取り入れたアプローチから解決策を探索している．人工知能学会 (JSAI) 会員．